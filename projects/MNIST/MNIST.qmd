
---
title: "Nearest neighbor for handwritten digit recognition"
jupyter: python3

---


In this project we will build a classifier that takes an image of a
handwritten digit and recognizes the digit present in the image. We
will look at a particularly simple strategy for this problem known as
the **nearest neighbor(NN) classifier**.


# The MNIST dataset

we will use `MNIST` dataset to learn our nearest neighbor
classifier.  `MNIST` is a classic dataset in machine learning,
consisting of 28x28 gray-scale images of handwritten digits.

Each data point i.e. handwritten digit image in the dataset is composed
of 28 x 28 i.e 784 pixels and is stored as a vector with 784
co-ordinates/dimensions, where each co-ordinate has a numeric value
ranging from 0 to 255.  Each image is also associated with a
corresponding label indicating the digit it represents.  The labels
range from 0 to 9, representing the ten possible digits.

If x is the vector of an image and y is the label:

-   Data space: $x \in \mathbb{R}^{784}$, a 784-dimensional vector
    consisting of numeric values from 0 to 255.
-   Label space: $y = \{0.....9\}$, the label of the image

The original `MNIST` training set contains 60,000 images and the test
set contains 10,000 images,  but in this project we will be working
with a subset of this data that was prepared before:    The
dataset consists of a training set of **7,500 images**, 750 images of
each digit   and a test set of **1,000 images**, 100 images of each
digit.

# Load in the modules and the dataset

```{python}
%matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import time


## Load in the training set

train_data = np.load('train_data.npy')
train_labels = np.load('train_labels.npy')

## Load in the testing set

test_data = np.load('test_data.npy')
test_labels = np.load('test_labels.npy')
```

Now lets see the dimensions and distribution of the dataset

## Dimensions of the training and the test set

```{python}
## Print out their dimensions

print("Training dataset dimensions: ", np.shape(train_data))
print("Number of training labels: ", len(train_labels), end='\n\n')

print("Testing dataset dimensions: ", np.shape(test_data))
print("Number of testing labels: ", len(test_labels))
```

## Compute the number of images of each digit

```{python}
## Compute the number of images of each digit


train_digits, train_counts = np.unique(train_labels, return_counts=True)
print("Training set distribution:")
print(dict(zip(train_digits, train_counts)), end='\n\n')

test_digits, test_counts = np.unique(test_labels, return_counts=True)
print("Test set distribution:")
print(dict(zip(test_digits, test_counts)))
      
      
```

So we have 750 images of 0-9 digit each with a total of 7500 images in
the training set   and 100 images of 0-9 digit each with a total of
1000 images in the test set.

# Visualize the data

To visualize a data point, we need to first reshape the 784-dimensional
vector into a 28x28 image.

we will define a function `vis_image()` where it in turn uses
`show_digit()` function   that displays an image of the digit when
vector representation form of a digit is given as input.

```{python}
## Define a function that displays a digit given its vector representation

# Each image i.e x is a vector in 784 coords / features----

def show_digit(x):
    
    plt.axis('off')
    plt.imshow(x.reshape((28,28)), cmap=plt.cm.gray)
    plt.show()


## Define a function that takes an index of particular data set ("train" or "test") and displays that image.

def vis_image(index, dataset="train"):
    
    if(dataset=="train"): 
        show_digit(train_data[index])
        label = train_labels[index]
    else:
        show_digit(test_data[index])
        label = test_labels[index]
    print("\t\t    Label " + str(label))

```

Now that we have defined a function for visualizing the image,   lets
test it by applying it on first data points in training set and test
set.

## View the first data point in the training set and test set

```{python}
## View the first data point in the training set

vis_image(0, "train")
```

```{python}
## Now view the first data point in the test set

vis_image(0, "test")
```

# Compute squared euclidean distance

To compute nearest neighbors in our data set, we need to first be able
to compute distances between data points (i.e. images here)   and the
most common, or default distance function is perhaps just Euclidean
distance.

The Euclidean distance between two 784-dimensional vectors
$x, z \in \mathbb{R}^{784}$ is:
$$\|x - z\| = \sqrt{\sum_{i=1}^{784} (x_i - z_i)^2}.$$

Often we omit the square root, and simply compute *squared Euclidean
distance*: $$\|x - z\|^2 = \sum_{i=1}^{784} (x_i - z_i)^2.$$

The following `squared_dist` function computes the squared Euclidean
distance.

```{python}
## Computes squared Euclidean distance between two vectors.

def squared_dist(x,z):
    return np.sum(np.square(x-z))

```

## Examples of computing squared euclidean distance

```{python}

print('Examples:\n')

## Computing distances between digits in our training set.

print(f"Distance from digit {train_labels[4]} to digit {train_labels[5]} in our training set: {squared_dist(train_data[4],train_data[5])}")


print(f"Distance from digit {train_labels[4]} to digit {train_labels[1]} in our training set: {squared_dist(train_data[4],train_data[1])}")


print(f"Distance from digit {train_labels[4]} to digit {train_labels[7]} in our training set: {squared_dist(train_data[4],train_data[7])}")
```

# Build nearest neighbor classifier

Now that we have a distance function defined, we can now turn to nearest
neighbor classification.

we define `find_NN()` and `NN_classifier()` functions   to find the
nearest neighbour image for a given image `x` i.e. the image that has
least squared euclidean distance and returns its label.

```{python}
## Takes a vector x and returns the index of its nearest neighbor in train_data

def find_NN(x):
    
    # Compute distances from x to every row in train_data
    
    distances = [squared_dist(x, train_data[i]) for i in range(len(train_labels))]
    
    # Get the index of the smallest distance
    return np.argmin(distances)


## Takes a vector x and returns the class of its nearest neighbor in train_data

def NN_classifier(x):
    
    # Get the index of the the nearest neighbor
    
    index = find_NN(x)
    
    # Return its class
    return train_labels[index]
```

# Apply NN classifier on full test set

Now let's apply our nearest neighbor classifier over the full test data
set. 

```{python}
## Predict on each test data point and time it!

#t_before = time.time()
#test_predictions = [NN_classifier(test_data[i]) for i in range(len(test_labels))]
#t_after = time.time()
```

## Compute the classification time of NN

Note that to classify each single test image in the test dataset of 1000
images,  NN classifier goes through the entire training set of 7500
images to find the NN image for that test image.

Thus we should not expect testing to be very fast.

```{python}
## Compute the classification time of NN classifier

#print(f"Classification time of NN classifier: {round(t_after - t_before, 2)} sec")
```

The code takes about 60-100 seconds on 1.70 GHz Intel Core i7 Laptop.

## Compute the error rate of NN

```{python}
#err_positions = np.not_equal(test_predictions, test_labels)
#error = float(np.sum(err_positions))/len(test_labels)

#print(f"Error rate of nearest neighbor classifier: {error * 100} %")
```

# Improving nearest neighbors

we found that NN classifier has given pretty reasonable performance, an
error rate of 4.06% on a separate test set.

we can improve the NN in two aspects:

-   Decreasing the error rate

-   Decreasing the classification time

    ```         
      Decreasing the error rate
    ```

Error rate can be decreased in two ways: using k-Nearest neighbors and
employing better distance functions

(i) k-Nearest neighbors

Instead of finding 1 nearest neighbor and returing its label, we can
find k nearest neighbors and return the majority label, where optimum
value for k is found by cross-validation technique.

(ii) Better distance functions

Using better distance functions like shape context and tangent distance
that are invariant to deformations could drastically reduce the error
rate as the Euclidean distance changes, if the image translates or
rotates slightly.

```         
    Decreasing the classification time
```

But in this project, we will focus on decreasing the classification
time.

we have seen that performing nearest neighbor classification in the way
we have presented requires a full pass through the training set in order
to classify a single point/image. If there are $N$ training points in
$\mathbb{R}^d$, this takes $O(N d)$ time.

Fortunately, there are faster methods to perform nearest neighbor search
if we are willing to spend some time preprocessing the training set,
where data structures are created. These data structures have names like
locality sensitive hashing, ball trees, K-d trees etc.

`scikit-learn` has fast implementations of two useful nearest neighbor
data structures: the *ball tree* and the *k-d tree*.

## Faster nearest neighbor methods:

### Ball tree algorithm

```{python}
#from sklearn.neighbors import BallTree

## Build nearest neighbor structure on training data
#t_before = time.time()
#ball_tree = BallTree(train_data)
#t_after = time.time()

## Compute training time
#t_training = t_after - t_before
#print("Time to build data structure (seconds): ", round(t_training, 2))

## Get nearest neighbor predictions on testing data
#t_before = time.time()
#test_neighbors = np.squeeze(ball_tree.query(test_data, k=1, return_distance=False))
#ball_tree_predictions = train_labels[test_neighbors]
#t_after = time.time()

## Compute testing time
#t_testing = t_after - t_before
#print("Time to classify test set (seconds): ", round(t_testing, 2))

## total classification time

#print("Overall classififcation time of Ball tree algorithm (seconds):", round(t_training+t_testing, 2), end = '\n\n')

## Verify that the predictions are the same
#print("Ball tree produces same predictions as NN? ", np.array_equal(test_predictions, ball_tree_predictions))
```

### k-d tree algorithm

```{python}
'''
from sklearn.neighbors import KDTree

## Build nearest neighbor structure on training data
t_before = time.time()
kd_tree = KDTree(train_data)
t_after = time.time()

## Compute training time
t_training = t_after - t_before
print("Time to build data structure (seconds): ", round(t_training, 2))

## Get nearest neighbor predictions on testing data
t_before = time.time()
test_neighbors = np.squeeze(kd_tree.query(test_data, k=1, return_distance=False))
kd_tree_predictions = train_labels[test_neighbors]
t_after = time.time()

## Compute testing time
t_testing = t_after - t_before
print("Time to classify test set (seconds): ", round(t_testing, 2))

## total classification time

print("Overall classififcation time of k-d tree algorithm (seconds):", round(t_training+t_testing, 2), end = '\n\n')

## Verify that the predictions are the same
print("KD tree produces same predictions as NN? ", np.array_equal(test_predictions, kd_tree_predictions))
'''
```
