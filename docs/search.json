[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home page",
    "section": "",
    "text": "About this site\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n Back to top"
  },
  {
    "objectID": "MNIST.html",
    "href": "MNIST.html",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "",
    "text": "In this project we will build a classifier that takes an image of a handwritten digit and recognizes the digit present in the image. We will look at a particularly simple strategy for this problem known as the nearest neighbor(NN) classifier."
  },
  {
    "objectID": "MNIST.html#the-mnist-dataset",
    "href": "MNIST.html#the-mnist-dataset",
    "title": "demosite",
    "section": "",
    "text": "MNIST is a classic dataset in machine learning, consisting of 28x28 gray-scale images handwritten digits. The original training set contains 60,000 examples and the test set contains 10,000 examples. In this notebook we will be working with a subset of this data: a training set of 7,500 examples and a test set of 1,000 examples.\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport time\n\n## Load the training set\ntrain_data = np.load('MNIST/train_data.npy')\ntrain_labels = np.load('MNIST/train_labels.npy')\n\n## Load the testing set\ntest_data = np.load('MNIST/test_data.npy')\ntest_labels = np.load('MNIST/test_labels.npy')\n## Print out their dimensions\n\nprint(\"Training dataset dimensions: \", np.shape(train_data))\nprint(\"Number of training labels: \", len(train_labels))\nprint(\"Testing dataset dimensions: \", np.shape(test_data))\nprint(\"Number of testing labels: \", len(test_labels))\n## Compute the number of examples of each digit\n\ntrain_digits, train_counts = np.unique(train_labels, return_counts=True)\nprint(\"Training set distribution:\")\nprint(dict(zip(train_digits, train_counts)), end='\\n\\n')\n\ntest_digits, test_counts = np.unique(test_labels, return_counts=True)\nprint(\"Test set distribution:\")\nprint(dict(zip(test_digits, test_counts)))"
  },
  {
    "objectID": "MNIST.html#visualizing-the-data",
    "href": "MNIST.html#visualizing-the-data",
    "title": "demosite",
    "section": "",
    "text": "Each data point is stored as 784-dimensional vector. To visualize a data point, we first reshape it to a 28x28 image.\n## Define a function that displays a digit given its vector representation\n\ndef show_digit(x):\n    plt.axis('off')\n    plt.imshow(x.reshape((28,28)), cmap=plt.cm.gray)\n    plt.show()\n    return\n\n## Define a function that takes an index into a particular data set (\"train\" or \"test\") and displays that image.\ndef vis_image(index, dataset=\"train\"):\n    \n\n    if(dataset==\"train\"): \n        show_digit(train_data[index,])\n        label = train_labels[index]\n    else:\n        show_digit(test_data[index,])\n        label = test_labels[index]\n        \n    print(\"Label \" + str(label))\n    return\n\n## View the first data point in the training set\nvis_image(0, \"train\")\n\n## Now view the first data point in the test set\nvis_image(0, \"test\")"
  },
  {
    "objectID": "MNIST.html#squared-euclidean-distance",
    "href": "MNIST.html#squared-euclidean-distance",
    "title": "demosite",
    "section": "",
    "text": "To compute nearest neighbors in our data set, we need to first be able to compute distances between data points. A natural distance function is Euclidean distance: for two vectors \\(x, y \\in \\mathbb{R}^d\\), their Euclidean distance is defined as \\[\\|x - y\\| = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}.\\] Often we omit the square root, and simply compute squared Euclidean distance: \\[\\|x - y\\|^2 = \\sum_{i=1}^d (x_i - y_i)^2.\\] For the purposes of nearest neighbor computations, the two are equivalent: for three vectors \\(x, y, z \\in \\mathbb{R}^d\\), we have \\(\\|x - y\\| \\leq \\|x - z\\|\\) if and only if \\(\\|x - y\\|^2 \\leq \\|x - z\\|^2\\).\nNow we just need to be able to compute squared Euclidean distance. The following function does so.\n## Computes squared Euclidean distance between two vectors.\ndef squared_dist(x,y):\n    return np.sum(np.square(x-y))\n\n## Compute distance between a seven and a one in our training set.\nprint(\"Distance from 7 to 1: \", squared_dist(train_data[4,],train_data[5,]))\n\n## Compute distance between a seven and a two in our training set.\nprint(\"Distance from 7 to 2: \", squared_dist(train_data[4,],train_data[1,]))\n\n## Compute distance between two seven's in our training set.\nprint(\"Distance from 7 to 7: \", squared_dist(train_data[4,],train_data[7,]))"
  },
  {
    "objectID": "MNIST.html#computing-nearest-neighbors",
    "href": "MNIST.html#computing-nearest-neighbors",
    "title": "demosite",
    "section": "",
    "text": "Now that we have a distance function defined, we can now turn to nearest neighbor classification.\n## Takes a vector x and returns the index of its nearest neighbor in train_data\ndef find_NN(x):\n    # Compute distances from x to every row in train_data\n    distances = [squared_dist(x,train_data[i,]) for i in range(len(train_labels))]\n    # Get the index of the smallest distance\n    return np.argmin(distances)\n\n## Takes a vector x and returns the class of its nearest neighbor in train_data\ndef NN_classifier(x):\n    # Get the index of the the nearest neighbor\n    index = find_NN(x)\n    # Return its class\n    return train_labels[index]\n## A success case:\n\nprint(\"A success case:\")\nprint(\"NN classification: \", NN_classifier(test_data[0,]))\nprint(\"True label: \", test_labels[0])\nprint(\"The test image:\")\nvis_image(0, \"test\")\nprint(\"The corresponding nearest neighbor image:\")\nvis_image(find_NN(test_data[0,]), \"train\")\n## A failure case:\nprint(\"A failure case:\")\nprint(\"NN classification: \", NN_classifier(test_data[39,]))\nprint(\"True label: \", test_labels[39])\nprint(\"The test image:\")\nvis_image(39, \"test\")\nprint(\"The corresponding nearest neighbor image:\")\nvis_image(find_NN(test_data[39,]), \"train\")"
  },
  {
    "objectID": "MNIST.html#processing-the-full-test-set",
    "href": "MNIST.html#processing-the-full-test-set",
    "title": "demosite",
    "section": "",
    "text": "Now let’s apply our nearest neighbor classifier over the full data set.\nNote that to classify each test point, our code takes a full pass over each of the 7500 training examples. Thus we should not expect testing to be very fast. The following code takes about 100-150 seconds on 2.6 GHz Intel Core i5.\n## Predict on each test data point (and time it!)\nt_before = time.time()\ntest_predictions = [NN_classifier(test_data[i,]) for i in range(len(test_labels))]\nt_after = time.time()\n\n## Compute the error\nerr_positions = np.not_equal(test_predictions, test_labels)\nerror = float(np.sum(err_positions))/len(test_labels)\n\nprint(\"Error of nearest neighbor classifier: \", error)\nprint(\"Classification time (seconds): \", t_after - t_before)"
  },
  {
    "objectID": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html",
    "href": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "",
    "text": "In this notebook we will build a classifier that takes an image of a handwritten digit and outputs a label 0-9. We will look at a particularly simple strategy for this problem known as the nearest neighbor classifier.\nTo run this notebook you should have the following Python packages installed: * numpy * matplotlib * sklearn\n\n\nMNIST is a classic dataset in machine learning, consisting of 28x28 gray-scale images handwritten digits. The original training set contains 60,000 examples and the test set contains 10,000 examples. In this notebook we will be working with a subset of this data: a training set of 7,500 examples and a test set of 1,000 examples.\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport time\n\n## Load the training set\ntrain_data = np.load('MNIST/train_data.npy')\ntrain_labels = np.load('MNIST/train_labels.npy')\n\n## Load the testing set\ntest_data = np.load('MNIST/test_data.npy')\ntest_labels = np.load('MNIST/test_labels.npy')\n## Print out their dimensions\n\nprint(\"Training dataset dimensions: \", np.shape(train_data))\nprint(\"Number of training labels: \", len(train_labels))\nprint(\"Testing dataset dimensions: \", np.shape(test_data))\nprint(\"Number of testing labels: \", len(test_labels))\nTraining dataset dimensions:  (7500, 784)\nNumber of training labels:  7500\nTesting dataset dimensions:  (1000, 784)\nNumber of testing labels:  1000\n## Compute the number of examples of each digit\n\ntrain_digits, train_counts = np.unique(train_labels, return_counts=True)\nprint(\"Training set distribution:\")\nprint(dict(zip(train_digits, train_counts)), end='\\n\\n')\n\ntest_digits, test_counts = np.unique(test_labels, return_counts=True)\nprint(\"Test set distribution:\")\nprint(dict(zip(test_digits, test_counts)))\nTraining set distribution:\n{0: 750, 1: 750, 2: 750, 3: 750, 4: 750, 5: 750, 6: 750, 7: 750, 8: 750, 9: 750}\n\nTest set distribution:\n{0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100, 7: 100, 8: 100, 9: 100}\n\n\n\nEach data point is stored as 784-dimensional vector. To visualize a data point, we first reshape it to a 28x28 image.\n## Define a function that displays a digit given its vector representation\n\ndef show_digit(x):\n    plt.axis('off')\n    plt.imshow(x.reshape((28,28)), cmap=plt.cm.gray)\n    plt.show()\n    return\n\n## Define a function that takes an index into a particular data set (\"train\" or \"test\") and displays that image.\ndef vis_image(index, dataset=\"train\"):\n    \n\n    if(dataset==\"train\"): \n        show_digit(train_data[index,])\n        label = train_labels[index]\n    else:\n        show_digit(test_data[index,])\n        label = test_labels[index]\n        \n    print(\"Label \" + str(label))\n    return\n\n## View the first data point in the training set\nvis_image(0, \"train\")\n\n## Now view the first data point in the test set\nvis_image(0, \"test\")\n\n\n\npng\n\n\nLabel 9\n\n\n\npng\n\n\nLabel 0\n\n\n\nTo compute nearest neighbors in our data set, we need to first be able to compute distances between data points. A natural distance function is Euclidean distance: for two vectors \\(x, y \\in \\mathbb{R}^d\\), their Euclidean distance is defined as \\[\\|x - y\\| = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}.\\] Often we omit the square root, and simply compute squared Euclidean distance: \\[\\|x - y\\|^2 = \\sum_{i=1}^d (x_i - y_i)^2.\\] For the purposes of nearest neighbor computations, the two are equivalent: for three vectors \\(x, y, z \\in \\mathbb{R}^d\\), we have \\(\\|x - y\\| \\leq \\|x - z\\|\\) if and only if \\(\\|x - y\\|^2 \\leq \\|x - z\\|^2\\).\nNow we just need to be able to compute squared Euclidean distance. The following function does so.\n## Computes squared Euclidean distance between two vectors.\ndef squared_dist(x,y):\n    return np.sum(np.square(x-y))\n\n## Compute distance between a seven and a one in our training set.\nprint(\"Distance from 7 to 1: \", squared_dist(train_data[4,],train_data[5,]))\n\n## Compute distance between a seven and a two in our training set.\nprint(\"Distance from 7 to 2: \", squared_dist(train_data[4,],train_data[1,]))\n\n## Compute distance between two seven's in our training set.\nprint(\"Distance from 7 to 7: \", squared_dist(train_data[4,],train_data[7,]))\nDistance from 7 to 1:  5357193.0\nDistance from 7 to 2:  12451684.0\nDistance from 7 to 7:  5223403.0\n\n\n\nNow that we have a distance function defined, we can now turn to nearest neighbor classification.\n## Takes a vector x and returns the index of its nearest neighbor in train_data\ndef find_NN(x):\n    # Compute distances from x to every row in train_data\n    distances = [squared_dist(x,train_data[i,]) for i in range(len(train_labels))]\n    # Get the index of the smallest distance\n    return np.argmin(distances)\n\n## Takes a vector x and returns the class of its nearest neighbor in train_data\ndef NN_classifier(x):\n    # Get the index of the the nearest neighbor\n    index = find_NN(x)\n    # Return its class\n    return train_labels[index]\n## A success case:\n\nprint(\"A success case:\")\nprint(\"NN classification: \", NN_classifier(test_data[0,]))\nprint(\"True label: \", test_labels[0])\nprint(\"The test image:\")\nvis_image(0, \"test\")\nprint(\"The corresponding nearest neighbor image:\")\nvis_image(find_NN(test_data[0,]), \"train\")\nA success case:\nNN classification:  0\nTrue label:  0\nThe test image:\n\n\n\npng\n\n\nLabel 0\nThe corresponding nearest neighbor image:\n\n\n\npng\n\n\nLabel 0\n## A failure case:\nprint(\"A failure case:\")\nprint(\"NN classification: \", NN_classifier(test_data[39,]))\nprint(\"True label: \", test_labels[39])\nprint(\"The test image:\")\nvis_image(39, \"test\")\nprint(\"The corresponding nearest neighbor image:\")\nvis_image(find_NN(test_data[39,]), \"train\")\nA failure case:\nNN classification:  2\nTrue label:  3\nThe test image:\n\n\n\npng\n\n\nLabel 3\nThe corresponding nearest neighbor image:\n\n\n\npng\n\n\nLabel 2\n\n\n\nThe above two examples show the results of the NN classifier on test points number 0 and 39.\nNow try test point number 100. * What is the index of its nearest neighbor in the training set? Record the answer: you will enter it as part of this week’s assignment. * Display both the test point and its nearest neighbor. * What label is predicted? Is this the correct label?\n\n\n\n\nNow let’s apply our nearest neighbor classifier over the full data set.\nNote that to classify each test point, our code takes a full pass over each of the 7500 training examples. Thus we should not expect testing to be very fast. The following code takes about 100-150 seconds on 2.6 GHz Intel Core i5.\n## Predict on each test data point (and time it!)\nt_before = time.time()\ntest_predictions = [NN_classifier(test_data[i,]) for i in range(len(test_labels))]\nt_after = time.time()\n\n## Compute the error\nerr_positions = np.not_equal(test_predictions, test_labels)\nerror = float(np.sum(err_positions))/len(test_labels)\n\nprint(\"Error of nearest neighbor classifier: \", error)\nprint(\"Classification time (seconds): \", t_after - t_before)\nError of nearest neighbor classifier:  0.046\nClassification time (seconds):  88.79683303833008\n\n\n\nPerforming nearest neighbor classification in the way we have presented requires a full pass through the training set in order to classify a single point. If there are \\(N\\) training points in \\(\\mathbb{R}^d\\), this takes \\(O(N d)\\) time.\nFortunately, there are faster methods to perform nearest neighbor look up if we are willing to spend some time preprocessing the training set. scikit-learn has fast implementations of two useful nearest neighbor data structures: the ball tree and the k-d tree.\nfrom sklearn.neighbors import BallTree\n\n## Build nearest neighbor structure on training data\nt_before = time.time()\nball_tree = BallTree(train_data)\nt_after = time.time()\n\n## Compute training time\nt_training = t_after - t_before\nprint(\"Time to build data structure (seconds): \", t_training)\n\n## Get nearest neighbor predictions on testing data\nt_before = time.time()\ntest_neighbors = np.squeeze(ball_tree.query(test_data, k=1, return_distance=False))\nball_tree_predictions = train_labels[test_neighbors]\nt_after = time.time()\n\n## Compute testing time\nt_testing = t_after - t_before\nprint(\"Time to classify test set (seconds): \", t_testing)\n\n## Verify that the predictions are the same\nprint(\"Ball tree produces same predictions as above? \", np.array_equal(test_predictions, ball_tree_predictions))\nTime to build data structure (seconds):  0.643275260925293\nTime to classify test set (seconds):  8.884220838546753\nBall tree produces same predictions as above?  True\nfrom sklearn.neighbors import KDTree\n\n## Build nearest neighbor structure on training data\nt_before = time.time()\nkd_tree = KDTree(train_data)\nt_after = time.time()\n\n## Compute training time\nt_training = t_after - t_before\nprint(\"Time to build data structure (seconds): \", t_training)\n\n## Get nearest neighbor predictions on testing data\nt_before = time.time()\ntest_neighbors = np.squeeze(kd_tree.query(test_data, k=1, return_distance=False))\nkd_tree_predictions = train_labels[test_neighbors]\nt_after = time.time()\n\n## Compute testing time\nt_testing = t_after - t_before\nprint(\"Time to classify test set (seconds): \", t_testing)\n\n## Verify that the predictions are the same\nprint(\"KD tree produces same predictions as above? \", np.array_equal(test_predictions, kd_tree_predictions))\nTime to build data structure (seconds):  1.15635347366333\nTime to classify test set (seconds):  12.445853233337402\nKD tree produces same predictions as above?  True"
  },
  {
    "objectID": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#the-mnist-dataset",
    "href": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#the-mnist-dataset",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "",
    "text": "MNIST is a classic dataset in machine learning, consisting of 28x28 gray-scale images handwritten digits. The original training set contains 60,000 examples and the test set contains 10,000 examples. In this notebook we will be working with a subset of this data: a training set of 7,500 examples and a test set of 1,000 examples.\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport time\n\n## Load the training set\ntrain_data = np.load('MNIST/train_data.npy')\ntrain_labels = np.load('MNIST/train_labels.npy')\n\n## Load the testing set\ntest_data = np.load('MNIST/test_data.npy')\ntest_labels = np.load('MNIST/test_labels.npy')\n## Print out their dimensions\n\nprint(\"Training dataset dimensions: \", np.shape(train_data))\nprint(\"Number of training labels: \", len(train_labels))\nprint(\"Testing dataset dimensions: \", np.shape(test_data))\nprint(\"Number of testing labels: \", len(test_labels))\nTraining dataset dimensions:  (7500, 784)\nNumber of training labels:  7500\nTesting dataset dimensions:  (1000, 784)\nNumber of testing labels:  1000\n## Compute the number of examples of each digit\n\ntrain_digits, train_counts = np.unique(train_labels, return_counts=True)\nprint(\"Training set distribution:\")\nprint(dict(zip(train_digits, train_counts)), end='\\n\\n')\n\ntest_digits, test_counts = np.unique(test_labels, return_counts=True)\nprint(\"Test set distribution:\")\nprint(dict(zip(test_digits, test_counts)))\nTraining set distribution:\n{0: 750, 1: 750, 2: 750, 3: 750, 4: 750, 5: 750, 6: 750, 7: 750, 8: 750, 9: 750}\n\nTest set distribution:\n{0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100, 7: 100, 8: 100, 9: 100}"
  },
  {
    "objectID": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#visualizing-the-data",
    "href": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#visualizing-the-data",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "",
    "text": "Each data point is stored as 784-dimensional vector. To visualize a data point, we first reshape it to a 28x28 image.\n## Define a function that displays a digit given its vector representation\n\ndef show_digit(x):\n    plt.axis('off')\n    plt.imshow(x.reshape((28,28)), cmap=plt.cm.gray)\n    plt.show()\n    return\n\n## Define a function that takes an index into a particular data set (\"train\" or \"test\") and displays that image.\ndef vis_image(index, dataset=\"train\"):\n    \n\n    if(dataset==\"train\"): \n        show_digit(train_data[index,])\n        label = train_labels[index]\n    else:\n        show_digit(test_data[index,])\n        label = test_labels[index]\n        \n    print(\"Label \" + str(label))\n    return\n\n## View the first data point in the training set\nvis_image(0, \"train\")\n\n## Now view the first data point in the test set\nvis_image(0, \"test\")\n\n\n\npng\n\n\nLabel 9\n\n\n\npng\n\n\nLabel 0"
  },
  {
    "objectID": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#squared-euclidean-distance",
    "href": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#squared-euclidean-distance",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "",
    "text": "To compute nearest neighbors in our data set, we need to first be able to compute distances between data points. A natural distance function is Euclidean distance: for two vectors \\(x, y \\in \\mathbb{R}^d\\), their Euclidean distance is defined as \\[\\|x - y\\| = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}.\\] Often we omit the square root, and simply compute squared Euclidean distance: \\[\\|x - y\\|^2 = \\sum_{i=1}^d (x_i - y_i)^2.\\] For the purposes of nearest neighbor computations, the two are equivalent: for three vectors \\(x, y, z \\in \\mathbb{R}^d\\), we have \\(\\|x - y\\| \\leq \\|x - z\\|\\) if and only if \\(\\|x - y\\|^2 \\leq \\|x - z\\|^2\\).\nNow we just need to be able to compute squared Euclidean distance. The following function does so.\n## Computes squared Euclidean distance between two vectors.\ndef squared_dist(x,y):\n    return np.sum(np.square(x-y))\n\n## Compute distance between a seven and a one in our training set.\nprint(\"Distance from 7 to 1: \", squared_dist(train_data[4,],train_data[5,]))\n\n## Compute distance between a seven and a two in our training set.\nprint(\"Distance from 7 to 2: \", squared_dist(train_data[4,],train_data[1,]))\n\n## Compute distance between two seven's in our training set.\nprint(\"Distance from 7 to 7: \", squared_dist(train_data[4,],train_data[7,]))\nDistance from 7 to 1:  5357193.0\nDistance from 7 to 2:  12451684.0\nDistance from 7 to 7:  5223403.0"
  },
  {
    "objectID": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#computing-nearest-neighbors",
    "href": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#computing-nearest-neighbors",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "",
    "text": "Now that we have a distance function defined, we can now turn to nearest neighbor classification.\n## Takes a vector x and returns the index of its nearest neighbor in train_data\ndef find_NN(x):\n    # Compute distances from x to every row in train_data\n    distances = [squared_dist(x,train_data[i,]) for i in range(len(train_labels))]\n    # Get the index of the smallest distance\n    return np.argmin(distances)\n\n## Takes a vector x and returns the class of its nearest neighbor in train_data\ndef NN_classifier(x):\n    # Get the index of the the nearest neighbor\n    index = find_NN(x)\n    # Return its class\n    return train_labels[index]\n## A success case:\n\nprint(\"A success case:\")\nprint(\"NN classification: \", NN_classifier(test_data[0,]))\nprint(\"True label: \", test_labels[0])\nprint(\"The test image:\")\nvis_image(0, \"test\")\nprint(\"The corresponding nearest neighbor image:\")\nvis_image(find_NN(test_data[0,]), \"train\")\nA success case:\nNN classification:  0\nTrue label:  0\nThe test image:\n\n\n\npng\n\n\nLabel 0\nThe corresponding nearest neighbor image:\n\n\n\npng\n\n\nLabel 0\n## A failure case:\nprint(\"A failure case:\")\nprint(\"NN classification: \", NN_classifier(test_data[39,]))\nprint(\"True label: \", test_labels[39])\nprint(\"The test image:\")\nvis_image(39, \"test\")\nprint(\"The corresponding nearest neighbor image:\")\nvis_image(find_NN(test_data[39,]), \"train\")\nA failure case:\nNN classification:  2\nTrue label:  3\nThe test image:\n\n\n\npng\n\n\nLabel 3\nThe corresponding nearest neighbor image:\n\n\n\npng\n\n\nLabel 2"
  },
  {
    "objectID": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#for-you-to-try",
    "href": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#for-you-to-try",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "",
    "text": "The above two examples show the results of the NN classifier on test points number 0 and 39.\nNow try test point number 100. * What is the index of its nearest neighbor in the training set? Record the answer: you will enter it as part of this week’s assignment. * Display both the test point and its nearest neighbor. * What label is predicted? Is this the correct label?"
  },
  {
    "objectID": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#processing-the-full-test-set",
    "href": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#processing-the-full-test-set",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "",
    "text": "Now let’s apply our nearest neighbor classifier over the full data set.\nNote that to classify each test point, our code takes a full pass over each of the 7500 training examples. Thus we should not expect testing to be very fast. The following code takes about 100-150 seconds on 2.6 GHz Intel Core i5.\n## Predict on each test data point (and time it!)\nt_before = time.time()\ntest_predictions = [NN_classifier(test_data[i,]) for i in range(len(test_labels))]\nt_after = time.time()\n\n## Compute the error\nerr_positions = np.not_equal(test_predictions, test_labels)\nerror = float(np.sum(err_positions))/len(test_labels)\n\nprint(\"Error of nearest neighbor classifier: \", error)\nprint(\"Classification time (seconds): \", t_after - t_before)\nError of nearest neighbor classifier:  0.046\nClassification time (seconds):  88.79683303833008"
  },
  {
    "objectID": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#faster-nearest-neighbor-methods",
    "href": "Nearest_neighbor_MNIST/Nearest_neighbor_MNIST backup.html#faster-nearest-neighbor-methods",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "",
    "text": "Performing nearest neighbor classification in the way we have presented requires a full pass through the training set in order to classify a single point. If there are \\(N\\) training points in \\(\\mathbb{R}^d\\), this takes \\(O(N d)\\) time.\nFortunately, there are faster methods to perform nearest neighbor look up if we are willing to spend some time preprocessing the training set. scikit-learn has fast implementations of two useful nearest neighbor data structures: the ball tree and the k-d tree.\nfrom sklearn.neighbors import BallTree\n\n## Build nearest neighbor structure on training data\nt_before = time.time()\nball_tree = BallTree(train_data)\nt_after = time.time()\n\n## Compute training time\nt_training = t_after - t_before\nprint(\"Time to build data structure (seconds): \", t_training)\n\n## Get nearest neighbor predictions on testing data\nt_before = time.time()\ntest_neighbors = np.squeeze(ball_tree.query(test_data, k=1, return_distance=False))\nball_tree_predictions = train_labels[test_neighbors]\nt_after = time.time()\n\n## Compute testing time\nt_testing = t_after - t_before\nprint(\"Time to classify test set (seconds): \", t_testing)\n\n## Verify that the predictions are the same\nprint(\"Ball tree produces same predictions as above? \", np.array_equal(test_predictions, ball_tree_predictions))\nTime to build data structure (seconds):  0.643275260925293\nTime to classify test set (seconds):  8.884220838546753\nBall tree produces same predictions as above?  True\nfrom sklearn.neighbors import KDTree\n\n## Build nearest neighbor structure on training data\nt_before = time.time()\nkd_tree = KDTree(train_data)\nt_after = time.time()\n\n## Compute training time\nt_training = t_after - t_before\nprint(\"Time to build data structure (seconds): \", t_training)\n\n## Get nearest neighbor predictions on testing data\nt_before = time.time()\ntest_neighbors = np.squeeze(kd_tree.query(test_data, k=1, return_distance=False))\nkd_tree_predictions = train_labels[test_neighbors]\nt_after = time.time()\n\n## Compute testing time\nt_testing = t_after - t_before\nprint(\"Time to classify test set (seconds): \", t_testing)\n\n## Verify that the predictions are the same\nprint(\"KD tree produces same predictions as above? \", np.array_equal(test_predictions, kd_tree_predictions))\nTime to build data structure (seconds):  1.15635347366333\nTime to classify test set (seconds):  12.445853233337402\nKD tree produces same predictions as above?  True"
  },
  {
    "objectID": "index.html#visualizing-the-data",
    "href": "index.html#visualizing-the-data",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "1.1 Visualizing the data",
    "text": "1.1 Visualizing the data\nEach data point is stored as 784-dimensional vector. To visualize a data point, we first reshape it to a 28x28 image.\n\n\nCode\n## Define a function that displays a digit given its vector representation\n\ndef show_digit(x):\n    plt.axis('off')\n    plt.imshow(x.reshape((28,28)), cmap=plt.cm.gray)\n    plt.show()\n    return\n\n## Define a function that takes an index into a particular data set (\"train\" or \"test\") and displays that image.\ndef vis_image(index, dataset=\"train\"):\n    \n\n    if(dataset==\"train\"): \n        show_digit(train_data[index,])\n        label = train_labels[index]\n    else:\n        show_digit(test_data[index,])\n        label = test_labels[index]\n        \n    print(\"Label \" + str(label))\n    return\n\n## View the first data point in the training set\nvis_image(0, \"train\")\n\n## Now view the first data point in the test set\nvis_image(0, \"test\")\n\n\n\n\n\nLabel 9\nLabel 0"
  },
  {
    "objectID": "index.html#squared-euclidean-distance",
    "href": "index.html#squared-euclidean-distance",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "1.2 Squared Euclidean distance",
    "text": "1.2 Squared Euclidean distance\nTo compute nearest neighbors in our data set, we need to first be able to compute distances between data points. A natural distance function is Euclidean distance: for two vectors \\(x, y \\in \\mathbb{R}^d\\), their Euclidean distance is defined as \\[\\|x - y\\| = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}.\\] Often we omit the square root, and simply compute squared Euclidean distance: \\[\\|x - y\\|^2 = \\sum_{i=1}^d (x_i - y_i)^2.\\] For the purposes of nearest neighbor computations, the two are equivalent: for three vectors \\(x, y, z \\in \\mathbb{R}^d\\), we have \\(\\|x - y\\| \\leq \\|x - z\\|\\) if and only if \\(\\|x - y\\|^2 \\leq \\|x - z\\|^2\\).\nNow we just need to be able to compute squared Euclidean distance. The following function does so.\n\n\nCode\n## Computes squared Euclidean distance between two vectors.\ndef squared_dist(x,y):\n    return np.sum(np.square(x-y))\n\n## Compute distance between a seven and a one in our training set.\nprint(\"Distance from 7 to 1: \", squared_dist(train_data[4,],train_data[5,]))\n\n## Compute distance between a seven and a two in our training set.\nprint(\"Distance from 7 to 2: \", squared_dist(train_data[4,],train_data[1,]))\n\n## Compute distance between two seven's in our training set.\nprint(\"Distance from 7 to 7: \", squared_dist(train_data[4,],train_data[7,]))\n\n\nDistance from 7 to 1:  5357193.0\nDistance from 7 to 2:  12451684.0\nDistance from 7 to 7:  5223403.0"
  },
  {
    "objectID": "index.html#computing-nearest-neighbors",
    "href": "index.html#computing-nearest-neighbors",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "1.3 Computing nearest neighbors",
    "text": "1.3 Computing nearest neighbors\nNow that we have a distance function defined, we can now turn to nearest neighbor classification.\n\n\nCode\n## Takes a vector x and returns the index of its nearest neighbor in train_data\ndef find_NN(x):\n    # Compute distances from x to every row in train_data\n    distances = [squared_dist(x,train_data[i,]) for i in range(len(train_labels))]\n    # Get the index of the smallest distance\n    return np.argmin(distances)\n\n## Takes a vector x and returns the class of its nearest neighbor in train_data\ndef NN_classifier(x):\n    # Get the index of the the nearest neighbor\n    index = find_NN(x)\n    # Return its class\n    return train_labels[index]\n\n\n\nA success caseCorresponding NN image\n\n\n\n\nCode\n## A success case:\n\nprint(\"A success case:\")\nprint(\"NN classification: \", NN_classifier(test_data[0,]))\nprint(\"True label: \", test_labels[0])\nprint(\"The test image:\")\nvis_image(0, \"test\")\n\n\nA success case:\nNN classification:  0\nTrue label:  0\nThe test image:\nLabel 0\n\n\n\n\n\n\n\n\n\nCode\nprint(\"The corresponding nearest neighbor image:\")\nvis_image(find_NN(test_data[0,]), \"train\")\n\n\nThe corresponding nearest neighbor image:\nLabel 0\n\n\n\n\n\n\n\n\n\n\nCode\n## A failure case:\nprint(\"A failure case:\")\nprint(\"NN classification: \", NN_classifier(test_data[39,]))\nprint(\"True label: \", test_labels[39])\nprint(\"The test image:\")\nvis_image(39, \"test\")\nprint(\"The corresponding nearest neighbor image:\")\nvis_image(find_NN(test_data[39,]), \"train\")\n\n\nA failure case:\nNN classification:  2\nTrue label:  3\nThe test image:\nLabel 3\nThe corresponding nearest neighbor image:\nLabel 2"
  },
  {
    "objectID": "index.html#processing-the-full-test-set",
    "href": "index.html#processing-the-full-test-set",
    "title": "demosite",
    "section": "Processing the full test set",
    "text": "Processing the full test set\nNow let’s apply our nearest neighbor classifier over the full data set.\nNote that to classify each test point, our code takes a full pass over each of the 7500 training examples. Thus we should not expect testing to be very fast. The following code takes about 100-150 seconds on 2.6 GHz Intel Core i5.\n\n## Predict on each test data point (and time it!)\nt_before = time.time()\ntest_predictions = [NN_classifier(test_data[i,]) for i in range(len(test_labels))]\nt_after = time.time()\n\n## Compute the error\nerr_positions = np.not_equal(test_predictions, test_labels)\nerror = float(np.sum(err_positions))/len(test_labels)\n\nprint(\"Error of nearest neighbor classifier: \", error)\nprint(\"Classification time (seconds): \", t_after - t_before)\n\nError of nearest neighbor classifier:  0.046\nClassification time (seconds):  43.372166872024536"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "MNIST.html#dimensions-of-the-training-and-the-test-set",
    "href": "MNIST.html#dimensions-of-the-training-and-the-test-set",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "2.1 Dimensions of the training and the test set",
    "text": "2.1 Dimensions of the training and the test set\n\n\nCode\n## Print out their dimensions\n\nprint(\"Training dataset dimensions: \", np.shape(train_data))\nprint(\"Number of training labels: \", len(train_labels), end='\\n\\n')\n\nprint(\"Testing dataset dimensions: \", np.shape(test_data))\nprint(\"Number of testing labels: \", len(test_labels))\n\n\nTraining dataset dimensions:  (7500, 784)\nNumber of training labels:  7500\n\nTesting dataset dimensions:  (1000, 784)\nNumber of testing labels:  1000"
  },
  {
    "objectID": "MNIST.html#compute-the-number-of-images-of-each-digit",
    "href": "MNIST.html#compute-the-number-of-images-of-each-digit",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "2.2 Compute the number of images of each digit",
    "text": "2.2 Compute the number of images of each digit\n\n\nCode\n## Compute the number of images of each digit\n\n\ntrain_digits, train_counts = np.unique(train_labels, return_counts=True)\nprint(\"Training set distribution:\")\nprint(dict(zip(train_digits, train_counts)), end='\\n\\n')\n\ntest_digits, test_counts = np.unique(test_labels, return_counts=True)\nprint(\"Test set distribution:\")\nprint(dict(zip(test_digits, test_counts)))\n\n\nTraining set distribution:\n{0: 750, 1: 750, 2: 750, 3: 750, 4: 750, 5: 750, 6: 750, 7: 750, 8: 750, 9: 750}\n\nTest set distribution:\n{0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100, 7: 100, 8: 100, 9: 100}\n\n\nSo we have 750 images of 0-9 digit each with a total of 7500 images in the training set and 100 images of 0-9 digit each with a total of 1000 images in the test set."
  },
  {
    "objectID": "MNIST.html#view-the-first-data-point-in-the-training-set-and-test-set",
    "href": "MNIST.html#view-the-first-data-point-in-the-training-set-and-test-set",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "3.1 View the first data point in the training set and test set",
    "text": "3.1 View the first data point in the training set and test set\n\n\nCode\n## View the first data point in the training set\n\nvis_image(0, \"train\")\n\n\n\n\n\n            Label 9\n\n\n\n\nCode\n## Now view the first data point in the test set\n\nvis_image(0, \"test\")\n\n\n\n\n\n            Label 0"
  },
  {
    "objectID": "MNIST.html#examples-of-computing-squared-euclidean-distance",
    "href": "MNIST.html#examples-of-computing-squared-euclidean-distance",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "4.1 Examples of computing squared euclidean distance",
    "text": "4.1 Examples of computing squared euclidean distance\n\n\nCode\nprint('Examples:\\n')\n\n## Computing distances between digits in our training set.\n\nprint(f\"Distance from digit {train_labels[4]} to digit {train_labels[5]} in our training set: {squared_dist(train_data[4],train_data[5])}\")\n\n\nprint(f\"Distance from digit {train_labels[4]} to digit {train_labels[1]} in our training set: {squared_dist(train_data[4],train_data[1])}\")\n\n\nprint(f\"Distance from digit {train_labels[4]} to digit {train_labels[7]} in our training set: {squared_dist(train_data[4],train_data[7])}\")\n\n\nExamples:\n\nDistance from digit 7 to digit 1 in our training set: 5357193.0\nDistance from digit 7 to digit 2 in our training set: 12451684.0\nDistance from digit 7 to digit 7 in our training set: 5223403.0"
  },
  {
    "objectID": "MNIST.html#compute-the-classification-time-of-nn",
    "href": "MNIST.html#compute-the-classification-time-of-nn",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "6.1 Compute the classification time of NN",
    "text": "6.1 Compute the classification time of NN\nNote that to classify each single test image in the test dataset of 1000 images, NN classifier goes through the entire training set of 7500 images to find the NN image for that test image.\nThus we should not expect testing to be very fast.\n\n\nCode\n## Compute the classification time of NN classifier\n\nprint(f\"Classification time of NN classifier: {round(t_after - t_before, 2)} sec\")\n\n\nClassification time of NN classifier: 75.92 sec\n\n\nThe code takes about 60-100 seconds on 1.70 GHz Intel Core i7 Laptop."
  },
  {
    "objectID": "MNIST.html#compute-the-error-rate-of-nn",
    "href": "MNIST.html#compute-the-error-rate-of-nn",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "6.2 Compute the error rate of NN",
    "text": "6.2 Compute the error rate of NN\n\n\nCode\nerr_positions = np.not_equal(test_predictions, test_labels)\nerror = float(np.sum(err_positions))/len(test_labels)\n\nprint(f\"Error rate of nearest neighbor classifier: {error * 100} %\")\n\n\nError rate of nearest neighbor classifier: 4.6 %"
  },
  {
    "objectID": "MNIST.html#faster-nearest-neighbor-methods",
    "href": "MNIST.html#faster-nearest-neighbor-methods",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "7.1 Faster nearest neighbor methods:",
    "text": "7.1 Faster nearest neighbor methods:\n\n7.1.1 Ball tree algorithm\n\n\nCode\nfrom sklearn.neighbors import BallTree\n\n## Build nearest neighbor structure on training data\nt_before = time.time()\nball_tree = BallTree(train_data)\nt_after = time.time()\n\n## Compute training time\nt_training = t_after - t_before\nprint(\"Time to build data structure (seconds): \", round(t_training, 2))\n\n## Get nearest neighbor predictions on testing data\nt_before = time.time()\ntest_neighbors = np.squeeze(ball_tree.query(test_data, k=1, return_distance=False))\nball_tree_predictions = train_labels[test_neighbors]\nt_after = time.time()\n\n## Compute testing time\nt_testing = t_after - t_before\nprint(\"Time to classify test set (seconds): \", round(t_testing, 2))\n\n## total classification time\n\nprint(\"Overall classififcation time of Ball tree algorithm (seconds):\", round(t_training+t_testing, 2), end = '\\n\\n')\n\n## Verify that the predictions are the same\nprint(\"Ball tree produces same predictions as NN? \", np.array_equal(test_predictions, ball_tree_predictions))\n\n\nTime to build data structure (seconds):  0.84\nTime to classify test set (seconds):  9.2\nOverall classififcation time of Ball tree algorithm (seconds): 10.04\n\nBall tree produces same predictions as NN?  True\n\n\n\n\n7.1.2 k-d tree algorithm\n\n\nCode\nfrom sklearn.neighbors import KDTree\n\n## Build nearest neighbor structure on training data\nt_before = time.time()\nkd_tree = KDTree(train_data)\nt_after = time.time()\n\n## Compute training time\nt_training = t_after - t_before\nprint(\"Time to build data structure (seconds): \", round(t_training, 2))\n\n## Get nearest neighbor predictions on testing data\nt_before = time.time()\ntest_neighbors = np.squeeze(kd_tree.query(test_data, k=1, return_distance=False))\nkd_tree_predictions = train_labels[test_neighbors]\nt_after = time.time()\n\n## Compute testing time\nt_testing = t_after - t_before\nprint(\"Time to classify test set (seconds): \", round(t_testing, 2))\n\n## total classification time\n\nprint(\"Overall classififcation time of k-d tree algorithm (seconds):\", round(t_training+t_testing, 2), end = '\\n\\n')\n\n## Verify that the predictions are the same\nprint(\"KD tree produces same predictions as NN? \", np.array_equal(test_predictions, kd_tree_predictions))\n\n\nTime to build data structure (seconds):  1.4\nTime to classify test set (seconds):  11.45\nOverall classififcation time of k-d tree algorithm (seconds): 12.85\n\nKD tree produces same predictions as NN?  True"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "NNoverview.html",
    "href": "NNoverview.html",
    "title": "Nearest neighbor for handwritten digit recognition",
    "section": "",
    "text": "Back to top"
  }
]